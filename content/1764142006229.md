**The Log-Trick** $$\nabla_\theta p_\theta(x) = p_\theta(x) \nabla_\theta \log p_\theta(x)$$ **Derivation of the Gradient of Expectation:**

$$\nabla_\theta \mathbb{E}_{x \sim p_\theta}[f(x)] = \nabla_\theta \int p_\theta(x) f(x) \, dx$$

$$= \int \nabla_\theta p_\theta(x) f(x) \, dx$$

$$= \int p_\theta(x) \nabla_\theta \log p_\theta(x) f(x) \, dx$$

$$= \mathbb{E}_{x \sim p_\theta} \left[ f(x) \nabla_\theta \log p_\theta(x) \right]$$

**Importance Sampling** want to compute the expectation of $f(x)$ under $p(x)$, but we can only sample from $q(x)$.
$$\mathbb{E}_{x \sim p}[f(x)] = \int f(x) p(x) \, dx$$

$$= \int f(x) \frac{p(x)}{q(x)} q(x) \, dx$$

$$= \mathbb{E}_{x \sim q} \left[ f(x) \frac{p(x)}{q(x)} \right]$$

### 1.

An MDP is defined by the tuple:$\mathbb{S}$: Set of states.$\mathbb{A}$: Set of actions.$p(s_0)$: Initial state distribution.

$p(s'|s,a)$: Markov kernel / Transition probability.$ $P(s'|s,a) = \mathbb{P}(S_{t+1}=s' | S_t=s, A_t=a)$$$r(s,a)$: Reward function.

**Matrix-Vector Notation**For finite states

$$V^{\pi} = r^{\pi} + \gamma T^{\pi}V^{\pi}​$$
,$$V^{\pi} = (I - \gamma T^{\pi})^{-1}r^{\pi}​$$

**Fixed Point Iteration：**Iterative method to compute $V^\pi$ without matrix inversion:

$$V_{t}^{\pi} = r^{\pi} + \gamma T^{\pi}V_{t-1}^{\pi} =: \mathcal{B}^{\pi}V_{t-1}^{\pi}$$

**Convergence:** $||V_{t}^{\pi} - V^{\pi}||_{\infty} \le \gamma^{t}||V_{0}^{\pi} - V^{\pi}||_{\infty}​$

**Contraction Property**：The Bellman operator $\mathcal{B}^{\pi}$ is a contraction mapping:

$$||\mathcal{B}^{\pi}V - \mathcal{B}^{\pi}V^{\prime}||_{\infty} \le \gamma||V - V^{\prime}||_{\infty}$$

**Bellman Optimality Equation (Bellman's Identity)**The value function of the optimal policy ($V^*$) satisfies:

$$V^{*}(s) = \max_{a} \left( r(s,a) + \gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)V^{*}(s^{\prime}) \right)$$

**Policy Iteration：** Alternates between evaluating a policy and improving it.
**Complexity:** $\mathcal{O}(|\mathbb{S}|^{3})$.Converges to the **exact** optimal policy.
**Policy Evaluation** 
$$V_{k+1}(s) \leftarrow \sum_{a} \pi(a|s) \sum_{s', r} p(s',r|s,a) [r + \gamma V_k(s')]$$

**Policy Improvement**
$$\pi'(s) \doteq \operatorname{argmax}_a q_{\pi}(s,a) = \operatorname{argmax}_a \sum_{s', r} p(s',r|s,a) [r + \gamma v_{\pi}(s')]$$

**Value Iteration：**Iteratively applies the Bellman optimality operator. Update Rule:

$$V_{t}(s) = \max_{a} \left( r(s,a) + \gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)V_{t-1}(s^{\prime}) \right)$$

**Complexity:** Quadratic per iteration $\mathcal{O}(|\mathbb{S}|^{2}|\mathbb{A}|)​$.
**Result:** Converges to an **$\epsilon$-optimal** solution.

$$v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s',r} p(s', r|s, a) \left[ r + \gamma v_{\pi}(s') \right], \quad \forall s \in \mathcal{S}​$$

$$q_{\pi}(s, a) = \sum_{s',r} p(s', r|s, a) \left[ r + \gamma \sum_{a'} \pi(a'|s') q_{\pi}(s', a') \right]​$$

$$v_{*}(s) = \max_{a} \sum_{s',r} p(s', r|s, a) \left[ r + \gamma v_{*}(s') \right]​$$

$$q_{*}(s, a) = \sum_{s',r} p(s', r|s, a) \left[ r + \gamma \max_{a'} q_{*}(s', a') \right]​$$

### 2.

**Markov Property:** The future depends only on the current state and action, not the history.
**Multi-Armed Bandits (MAB)**  **Problem:** Sequential selection of arms $a_t$ to maximize cumulative reward $R_t$ from unknown distributions $p_{a_t}$. **Exploration-Exploitation Conflict:** Balancing gathering information (exploration) vs. maximizing immediate reward (exploitation).

**Cumulative Regret** ($\rho_T$): Difference between expected reward of optimal arm and expected reward of chosen arms.

$$\rho_{T} = \max_{a}\sum_{t=1}^{T}\mathbb{E}[r_{a}] - \sum_{t=1}^{T}\mathbb{E}[r_{a_{t}}]$$

**1.Explore-First**Strategy: Explore each arm $N_a$ times, then exploit the best average.

**Hoeffding Inequality (Bound):**

$$P\left(|\mu(a) - \bar{\mu}(a)| \le \sqrt{\frac{2 \log(T)}{N_a}}\right) \ge 1 - \frac{2}{T^4}$$

Regret Bound: $\mathbb{E}[\rho_{T}] \le \mathcal{O}(KT^{2/3}(\log(T))^{1/3})$.

**2.Epsilon-Greedy ($\epsilon$-Greedy)**Strategy: With probability $\epsilon$, explore random arm; else exploit best estimated arm.
Incremental Update Rule:

$$\bar{\mu}_{N_{a}+1}(a) = \bar{\mu}_{N_{a}}(a) + \frac{1}{N_{a}}(R_{N_{a}} - \bar{\mu}_{N_{a}}(a))$$

General Step Size Update:

$$\text{NewEst} = \text{OldEst} + \text{StepSize} \times (\text{Target} - \text{OldEst})$$

**Robbins-Monro Convergence Conditions:** $\sum \alpha_N = \infty$ and $\sum \alpha_N^2 < \infty$.

**3.Upper Confidence Bound (UCB)**Strategy: Optimism in the face of uncertainty. Select arm maximizing the UCB. Selection Formula:

$$a_t = \arg\max_a \left( \bar{\mu}(a) + c \sqrt{\frac{\log(T)}{N_a}} \right)$$

Regret Bound: $\mathbb{E}[\rho_{T}] \le \mathcal{O}(\sqrt{KT \log(T)})$.

**4.Gradient Bandits (Random Policies)**Strategy: Learn preferences $H_t(a)$ using Softmax (Boltzmann) distribution.

Policy Probability，Boltzmann Distribution:$$\pi_{t}(a) = \frac{e^{H_{t}(a)}}{\sum_{k=1}^{K}e^{H_{t}(k)}}$$

Stochastic Gradient Ascent Update (with baseline $\bar{R}_t$):

$$H_{t+1}(a) = H_{t}(a) + \alpha(R_{t} - \bar{R}_{t})(\mathbb{I}_{A_{t}=a} - \pi_{t}(a))$$

**Learning the Model：** Transition Probability Estimate (Counting):

$$\hat{P}(s'|s,a) = \frac{N(s',s,a)}{\sum_{s''} N(s'',s,a)}​$$

Reward Function Estimate:$$\hat{r}(s,a) = \frac{\sum R_t}{N(s,a)}​$$

**$\epsilon$-Greedy in MDPs:** Acts randomly with prob $\epsilon$, otherwise greedy w.r.t. learned model.

**$R_{max}$ Algorithm (Optimism):**
Initializes unknown rewards to $R_{max}$ (upper bound). Unknown transitions point to a hypothetical "fairy tale" state $S^*$ with maximum reward. **Implicit exploration:** The agent is attracted to unknown states because it assumes they yield high rewards.

### 3.

**Random Policies (Boltzmann):** Sample actions based on a learned preference distribution.
**Regret Bound:**

$$\rho_{T} \ge T\frac{(K-1)}{K} \min_{a \ne a'} \mathbb{E}[r_{a} - r_{a'}]​$$

Empirical Mean:$$\mu(a) \approx \frac{\sum_{t=1}^{N_{a}}R_{t}}{N_{a}} = \overline{\mu}(a)$$

Epsilon-Greedy Decay Rate:
$$\epsilon_{t} = t^{-\frac{1}{3}}(K \log(t))^{1/3}$$

Upper Confidence Bound (UCB):

$$ucb(a) = \overline{\mu}(a) + \sqrt{\frac{2 \log(T)}{N_{a}}}$$




**Temporal Difference (TD) Learning**TD(0) Update Rule:

$$V(S_{t}) \leftarrow V(S_{t}) + \alpha_{t}(R_{t} + \gamma V(S_{t+1}) - V(S_{t}))$$

**SARSA:** State-Action-Reward-State-Action. **On-Policy:** Learns the value of the policy currently being followed (including exploration steps).

$$Q(S_{t},A_{t}) \leftarrow Q(S_{t},A_{t}) + \alpha_{t}(R_{t} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_{t},A_{t}))$$

**Q-Learning:** Uses the greedy action in the update target ($max_{a'}$).
**Off-Policy:** Learns the optimal policy $Q^*$ while following a different behavior policy (e.g., $\epsilon$-greedy).

$$Q(S_{t},A_{t}) \leftarrow Q(S_{t},A_{t}) + \alpha_{t}(R_{t} + \gamma \max_{a'} Q(S_{t+1},a') - Q(S_{t},A_{t}))$$

**Function Approximation:** Replaces tabular values with a parameterized function $V(s;\theta)$ (e.g., Neural Network) to handle large or continuous state spaces.

**DQN Heuristics**：**1.Experience Replay:** Store transitions in a buffer and sample minibatches to break correlation.**Target Network:** Use a frozen copy of parameters ($\theta^{old}$) for the target value to stabilize training.

**TD Update as SGD:**

$$V^{\pi}(s;\theta) \leftarrow V^{\pi}(s;\theta) + \alpha_{t}(r + \gamma V^{\pi}(s';\theta^{old}) - V^{\pi}(s;\theta))$$

**Q-Learning Loss Function:**

$$l(\theta;s,a) = \frac{1}{2}(r(s,a) + \gamma \max_{a'} Q^{*}(s',a';\theta^{old}) - Q^{*}(s,a;\theta))^2$$

**DQN Gradient Update:**

$$\theta \leftarrow \theta + \alpha_{t}\delta_{B}(s,a,s')\nabla_{\theta}Q^{*}(s,a;\theta)$$(Where $\delta_{B}$ is the Bellman error)

### 7.

**A. Reward Shaping**Define a reward function that assigns large negative values to unsafe behavior.

Formula:$$r(s,a) = r_{task}(s,a) + r_{safety}(s,a)​$$

**B. Risk-sensitive RL**Optimizes for risk tails (low-probability events) rather than just expected value.**Pros/Cons:** Easy to apply standard RL; but has no strong guarantees and requires complicated reward design. **Risk Objective:**

$$V^{\pi}(s) = \mathbb{R}_{\beta}^{*}[\sum_{k=0}^{\infty}\gamma^{k}r(s_{k},\pi(s_{k}))]​$$

**Metrics:** Conditional Value at Risk (CVaR), Entropic risk.

**Constrained MDPs (CMDP)**Explicitly represents safety conditions as constraints.

Policy Optimization Problem:

Maximize $\pi(s) = \arg\max_{a}(r_{task}(s,a) + \gamma\mathbb{E}_{s^{\prime}}[V_{task}^{\pi}(s^{\prime})])$  Subject to:

$$r_{safety}(s,a) + \gamma\mathbb{E}_{s^{\prime}}[V_{safety}^{\pi}(s^{\prime})] \ge c$$

Safety Value Function:

$$V_{safety}^{\pi}(s) = r_{safety}(s,a) + \gamma\mathbb{E}_{s^{\prime}}[V_{safety}^{\pi}(s^{\prime})]$$

**D. Constrained Policy Optimization (CPO)**Trust-region optimization for CMDPs.

Formulation:Maximize $\mathbb{E} [A_{task}(s, a)]$ Subject to:

1.Safety: $\mathbb{E}[V_{safety}^{\pi}(s_{0})] - \frac{1}{1-\gamma}\mathbb{E}[A_{safety}^{\pi}(s,a)] \ge c$

2.Trust Region (KL-divergence): $\overline{D}_{KL}(\pi||\pi_{k}) \le \delta$

**E. Primal-Dual Method (Lagrangian Relaxation)** Solves the CMDP by converting it to a dual problem (min-max).
**Lagrangian:**

$$d(\lambda) = \max_{\pi}L(\pi,\lambda) = \max_{\pi}\mathbb{E}[V_{task}(s,a)] + \lambda(\mathbb{E}[V_{safety}(s,a)] - c)$$

Optimization:$$\min_{\lambda\ge0}d(\lambda)$$
Gradient descent for $\lambda$. Gradient ascent for $\pi$.

**Control Barrier Functions (CBF)**  A function $h(x)$ that is positive inside the constraint set, 0 at the boundary, and negative outside. Used as a safety filter/shield.

Safety Condition:$$\mathbb{E}[h(s^{\prime})] \ge \alpha h(s)$$  where $\alpha \in [0,1]​$.

Safety Filter (Optimization):
$$\pi_{safe}(s) = \min_{a} ||a - \pi(s)||^{2}$$
Subject to: $\mathbb{E}[h(s^{\prime})] \ge \alpha h(s)$

**Learning:** CBFs can be learned from expert demonstrations or RL policy roll-outs.

**Constrained Predictive Control (MPC)**Online optimization over a sequence of actions (Model Predictive Control). Ensures strict constraint satisfaction.

Formula:$$\min_{a_{0},...,a_{K}}\mathbb{E}[\sum_{k=0}^{K}\gamma^{k}r(s_{k},a_{k}) + V(s_{K+1})]$$
Subject to: $s_{k} \in \mathbb{S}_{safe}, a_{k} \in \mathbb{A}_{safe}$

**Solvers:** Constrained cross-entropy method, Sequential quadratic programming, Non-convex collocation.

**Lagrangian Update Rule**

$$L(\pi, \lambda) = J_r(\pi) - \lambda (J_c(\pi) - d_{limit})$$

$$\lambda_{k+1} = \max(0, \lambda_k + \eta (J_c(\pi_k) - d_{limit}))$$

### 8.

**Partial Observability:** The agent does not have full information about the state ($s$); it only receives observations ($o$). Single observations are often insufficient for decision-making.

**POMDP (Partially Observable MDP):** An MDP augmented with a set of observations and observation probabilities.  **Belief State ($b_k$):** A probability distribution over possible states given the history of actions and observations. It acts as a sufficient statistic for the history.**Planning with Belief States:** Using Bayesian updates to estimate states (similar to Kalman filtering).
**Maximum Likelihood Estimation (MLE) Approximation:** A simplification where the agent acts based on the most likely state in the belief distribution rather than the full distribution.

Belief State:$$b_{k}=p(s_{k}|o_{0},a_{0},...,a_{k-1},o_{k})$$

Belief State Update:$$b_{k}=g(b_{k-1},a_{k-1},o_{k})​$$

**POMDP Objective (Fully Observable vs. Partially Observable):**

Fully Observable: $\mathbb{E}[\sum_{k=0}^{\infty}\gamma^{k}r(s_{k},a_{k})]$

Partially Observable (Belief-based):

$$\mathbb{E}[\sum_{k=0}^{\infty}\sum_{s\in S}\gamma^{k}b_{k}(s)r(s_{k},a_{k})]$$

Constrained Predictive Control (MPC approach):

$$\min_{a_{0},...,a_{K}}\mathbb{E}[\sum_{k=0}^{K}\gamma^{k}r(s_{k},a_{k})+V(s_{K+1})]$$

**Visuomotor Learning:** RL where observations are high-dimensional images.

### 9.

 **1. Inverse Reinforcement Learning (IRL)**
；**Goal:** Infer the unknown reward function using expert demonstration data, assuming the expert is optimal.**Challenges:** Reward ambiguity (trivial solution where reward is 0), dependence on expert optimality, and computational complexity.**Feature Matching:** If a policy matches the expert's expected feature counts, their value is close to the expert's value. 

**Max-Margin IRL:** Finds a reward function that maximizes the margin between the expert policy and other policies.
 **Max-Entropy IRL:** Parameterizes path probabilities and maximizes entropy to resolve reward ambiguity (using a Boltzmann distribution over paths).

**IRL Assumption** (Expert Optimality):

$$\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t})|\pi^{*}]\ge\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t})|\pi] \quad \forall\pi​$$

Linear Reward Parameterization: $$r(s)=w^{T}\phi(s)$$

**Max-Margin Optimization:**

$$min_{w,\xi}||w||^{2}+c\xi$$  Subject to:

$${w^{*}}^{T}\mu(\pi^{*})\ge{w^{*}}^{T}\mu(\pi)+m(\pi,\pi^{*})-\xi​$$

Feature Expectation Matching Bound:

$$||\mu(\pi)-\mu(\pi^{*})||_{1} \le \epsilon \implies |{w^{*}}^{T}\mu(\pi)-{w^{*}}^{T}\mu(\pi^{*})|\le\epsilon$$

**Max-Entropy Objective:**
$$max_{P}-\sum_{\zeta}P(\zeta)log~P(\zeta)$$

Subject to:$$\sum_{\zeta}P(\zeta)\mu(\zeta)=\mu(\pi^{*})$$

Max-Entropy Distribution Solution:
$$P(\zeta)=\frac{1}{Z(\theta)}exp(w^{T}\mu(\zeta))​$$

**2. Imitation Learning (Behavior Cloning)**

 **Behavior Cloning (BC):** Frames policy learning as a supervised regression/classification problem using a fixed dataset of expert trajectories.
 **Loss Functions:** Typically negative log-likelihood or square loss.
 **Distribution Shift:** The primary failure mode of BC. The training distribution comes from the expert ($p_{\pi^*}$), but the test distribution comes from the trained policy ($p_{\hat{\pi}^*}$).
 **Compounding Errors:** Small errors accumulate, leading the agent into states not seen during training.

**Formulas:** BC Optimization:
  $$\hat{\pi}^{*}=arg~min_{\pi}\sum_{i=1}^{N}l(\pi,s_{i},a_{i})$$

 BC Approximation: $$\approx arg~min\mathbb{E}_{s\sim p_{\pi^{*}}}[l(\pi,s,\pi^{*}(s))]$$

 Compounding Error Bound:  Returns drop by at most $\epsilon T^{2}$

**3. On-Policy Imitation Learning (DAgger)**

 **DAgger (Dataset Aggregation):** An iterative algorithm to address distribution shift.
 **Process:** 1 Train policy on current data.
  2 Rollout a mixture of the expert and learned policy.
  3 Query the expert to label new states.
  4 Aggregate data and retrain.
 **Result:** Reduces the error bound significantly compared to standard behavior cloning.

Policy Mixing:$$\pi_{i}=\beta_{i}\pi^{*}+(1-\beta_{i})\hat{\pi}$$

**Data Aggregation**:$$\mathcal{D}\leftarrow\mathcal{D}\cup\mathcal{D}_{i}$$
DAgger Error Bound: Returns drop by at most $\epsilon T$ (linear in time horizon).

**4.Diffusion Models in Robotics**

**Multi-modality:** Standard policies (e.g., Gaussian) struggle with multi-modal behavior; generative models solve this.**Diffusion Process:** Interprets imitation learning as fitting a probability distribution. It models trajectory generation as a reverse denoising process from Gaussian noise.**Planning:** Diffusion can be used for planning by conditioning on goals or initial states (inpainting).

**Reverse (Denoising) Process:**
$$p_{\theta}(\tau_{0:K}) = p(\tau_K) \prod_{k=1}^K p_{\theta}(\tau_{k-1}|\tau_k)$$

Modeled as transitions like: $\mathcal{N}(\mu_{\theta}(\tau_{K}),\Sigma_{K}) \to \dots \to \mathcal{N}(\mu_{\theta}(\tau_{1}),\Sigma_{1})$

Forward (Diffusion) Step:
$$q(\tau_k|\tau_{k-1}) = \mathcal{N}(\sqrt{1-\beta_{k}}\tau_{k-1}, \beta_{k}I)$$

### 10.

**1. Q-Learning** **Pros:** Simple, stable **Cons:** Discrete only
**2. REINFORCE** **Pros:** Works in continuous spaces **Cons:** High variance
**3. Actor-Critic** **Pros:** Lower variance, more efficient **Cons:** Still unstable

DQN Loss Function (MSE):

$$l(\theta;s,a) = \frac{1}{2} \left( r + \gamma \max_{a'} Q(s', a'; \theta^{old}) - Q(s,a;\theta) \right)^2$$

*Target:* $r + \gamma \max_{a'} Q(s', a'; \theta^{old})$

**Policy-Based Methods (REINFORCE)**

Objective Function: $$J(\theta) = \mathbb{E}_{\pi_{\theta}}[\sum_{t} \gamma^{t} r_{t}]$$

**Policy Gradient Theorem:**
$$\nabla_{\theta}J(\theta) = \mathbb{E}_{\pi_{\theta}}[\nabla_{\theta} \log \pi_{\theta}(a|s) Q^{\pi_{\theta}}(s,a)]$$
在 REINFORCE 中使用 $G_t$ 作为 $Q(s,a)$ 的无偏估计，但方差很大。引入 Critic 估计 $Q(s,a)$ 或优势函数 $A(s,a) = Q(s,a) - V(s)$ 可以显著降低方差

REINFORCE Update Rule (using Return $G_t$ instead of $Q$):

$$\nabla_{\theta}J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_{t}|s_{t}) G_{t}$$

$$\theta \leftarrow \theta + \alpha \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) G_t$$

**Actor-Critic Gradient:**
$$\nabla_{\theta}J(\theta) = \mathbb{E}_{\pi_{\theta}}[\nabla_{\theta} \log \pi_{\theta}(a|s) A^{\pi_{\theta}}(s,a)]$$

Advantage Function ($A$): Represents how much better a specific action is compared to the average value of the state.

$$A(s,a) = Q(s,a) - V(s)$$

Baseline Property: Subtracting a state-dependent baseline $b(s)$ (like $V(s)$) does not bias the gradient but reduces variance:
$$\mathbb{E}_{\pi_{\theta}}[\nabla_{\theta} \log \pi_{\theta}(a|s) b(s)] = 0$$

### 11.

**Soft Actor-Critic (SAC)**：
**Concept:** Off-policy, entropy-regularized RL. Adds an entropy bonus to the objective to encourage exploration and prevent premature convergence.

**Entropy-Regularized Objective:**
$$J(\theta)=E_{\pi_{\theta}}[\Sigma_{t}\gamma^{t}(r_{t}+\alpha\mathcal{H}(\pi(a|s_{t})))]$$

Entropy: $$\mathcal{H}(\pi(a|s))=-E[log~\pi(a|s)]$$

Soft Bellman Equation (Value):
$$V^{\pi}(s)=E_{a\sim\pi}[Q^{\pi}(s,a)-\alpha log~\pi(a|s)]$$

**Key Features:** Stochastic Gaussian actor, uses Replay Buffer, $\alpha$ (temperature) controls exploration.

**Deep Deterministic Policy Gradient (DDPG)**：
**Concept:** Off-policy method for continuous control using a deterministic actor.
**Update Rule**: Learns via gradients from a critic.

$$\nabla_{\theta}J(\theta)=E_{s\sim D}[\nabla_{a}Q_{\phi}(s,a)\nabla_{\theta}\pi_{\theta}(s)]$$

**Key Features:** Deterministic actor ($a=\pi_{\theta}(s)​$), requires noise injection for exploration, uses Replay Buffer and Target Networks.

**Trust Region Policy Optimization (TRPO)**：
**Concept:** On-policy method that constrains policy updates to a "Trust Region" to ensure monotonic improvement and stability.**Objective:** Maximize surrogate objective subject to a "Hard" KL-divergence constraint.

*Maximize:* $$E_{s,a\sim\pi_{old}}[\frac{\pi_{\theta}(a|s)}{\pi_{old}(a|s)}A^{\pi_{old}}(s,a)]​$$

*Subject to:* 
$$E_{s\sim\pi_{old}}[D_{KL}(\pi_{old}(\cdot|s)||\pi_{\theta}(\cdot|s))]\le\delta​$$

Update Rule (Natural Gradient): Solved via conjugate gradient and Hessian ($H$) of the KL divergence.
$$\theta_{k+1}=\theta_{k}+\sqrt{\frac{2\delta}{g^{T}H^{-1}g}}H^{-1}g​$$
(where $g​$ is the policy gradient and $H​$ is the Fisher Information Matrix)

**Proximal Policy Optimization (PPO)**：
**Concept:** A lightweight, first-order approximation of TRPO. Replaces the complex KL constraint/Hessian with a simple clipping mechanism ("Soft" constraint).

Probability Ratio:$$r_{t}(\theta)=\frac{\pi_{\theta}(a|s)}{\pi_{old}(a|s)}$$

Clipped Objective: Prevents the ratio $r_t​$ from deviating too far from 1.

$$L^{CLIP}(\theta)=E[min(r_{t}(\theta)A_{t}, clip(r_{t}(\theta), 1 - \epsilon , 1 + \epsilon )A_t)]$$

**Total Loss Function:**
$$L_{Total}(\theta)=L^{CLIP}(\theta)+c_{1}L^{VF}-c_{2}\mathcal{H}(\pi)​$$

(Includes Value Function loss $L^{VF}$ and Entropy bonus $\mathcal{H}$)

**Generalized Advantage Estimation (GAE)**：
**Concept:** A technique used (commonly with PPO) to balance bias and variance in advantage estimation.

TD Error:$$\delta_{t}=r_{t}+\gamma~V(S_{t+1})-V(S_{t})​$$

GAE Formula: Exponentially weighted average of TD errors.

$$\hat{A_{t}}=\Sigma_{l=0}^{\infty}(\gamma\lambda)^{l}\delta_{t+l}​$$  (Parameter $\lambda​$ controls the bias-variance trade-off)

 **Limitations of PPO & Modern Challenges** ：**On-policy Inefficiency:** Discards samples after one update; poor sample efficiency compared to off-policy methods (like SAC).**Clipping Heuristic:** The clipped ratio is a loose approximation of the KL constraint; does not guarantee true safety.**Hyperparameter Sensitivity:** Performance varies heavily based on clipping range ($\epsilon​$), learning rate, and batch size.**Value Function Lag:** The critic often learns slower than the actor, leading to stale advantage estimates.**Limited Exploration:** Relies on random noise; struggles in sparse-reward environments without intrinsic curiosity.


推导策略 $\pi$ 下的状态价值函数 $V^\pi(s)$ 的贝尔曼方程（递归形式）

价值函数定义为：

$V^\pi(s) = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r(S_t, \pi(S_t)) | S_0 = s]$ 。

$$V^\pi(s) = \mathbb{E}[r(s, \pi(s)) + \sum_{t=1}^{\infty} \gamma^t r(S_t, \pi(S_t)) | S_0 = s]$$

$$= r(s, \pi(s)) + \gamma \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k r(S_{k+1}, \pi(S_{k+1})) | S_0 = s] \quad $$ (令 k=t-1)

$$= r(s, \pi(s)) + \gamma \sum_{s'} p(s'|s, \pi(s)) \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k r(S_{k+1}, \pi(S_{k+1})) | S_1 = s']$$

$$= r(s, \pi(s)) + \gamma \sum_{s'} p(s'|s, \pi(s)) V^\pi(s')$$

策略迭代：通常在较少的迭代次数内收敛到精确解，但每一步需要进行策略评估（求解线性方程组或多次迭代），单步计算代价为 $O(|\mathcal{S}|^3)$ 。适用于状态空间较小的情况。价值迭代：每一步计算量较小 $O(|\mathcal{S}|^2|\mathcal{A}|)$，需要更多次迭代才能收敛到 $\epsilon$-最优解。

Proof:Expectation of $h(s')$: Let $s' = \mu + \epsilon$ where $\epsilon \sim \mathcal{N}(0, 0.1I)$ and $s \in \mathbb{R}^2$.

$$\mathbb{E}[h(s')] = 1 - \mathbb{E}[(\mu+\epsilon)^\top(\mu+\epsilon)] = 1 - \|\mu\|^2 - \text{Tr}(\text{Cov}(\epsilon))$$

$$\mathbb{E}[h(s')] = 1 - \|\mu\|^2 - 0.2 \quad (\text{since } \text{Tr}(0.1I_{2\times2}) = 0.2)$$

Calculate Mean $\mu$: Given $a = -[0.75, 1.0]s$, $A = [[0, 0.5], [0.75, 1]]$, $B = [0, 1]^\top$.$$\mu = As + Ba = \begin{bmatrix} 0.5s_2 \\ 0.75s_1 + s_2 \end{bmatrix} + \begin{bmatrix} 0 \\ -0.75s_1 - s_2 \end{bmatrix} = \begin{bmatrix} 0.5s_2 \\ 0 \end{bmatrix}$$Verify Inequality: We need to show $0.8 - (0.5s_2)^2 \ge 0.25(1 - s_1^2 - s_2^2)$.$$0.8 - 0.25s_2^2 \ge 0.25 - 0.25s_1^2 - 0.25s_2^2$$

$$0.55 + 0.25s_1^2 \ge 0$$This holds true for any real $s_1$. Thus, the CBF condition is satisfied.

**Safety using Lagrangian Relaxations** 使用 Primal-Dual 方法解决 CMDP。

 $\lambda$ 范围：Lagrange 乘子 $\lambda$ 用于将约束违规转化为惩罚。根据 KKT 条件或对偶问题定义，$\lambda$ 必须是非负的，即 $\lambda \ge 0$。
**$\lambda$ 更新：**通常约束形式为 $V_{safety} \ge c$ (安全值要高) 或 $Cost \le c$ (代价要低)。题目中 $V_{safety}=0.5 < c=1.0$。假设约束是 $V_{safety} \ge c$，则当前违规。对偶更新（Gradient Ascent on Dual）：$\lambda \leftarrow \lambda - \eta \nabla_\lambda L$。
Lagrangian $L = V_{task} + \lambda (V_{safety} - c)$。
$\lambda_{new} = \lambda - \eta (V_{safety} - c) = 5 - 0.1 \times (0.5 - 1.0) = 5 - 0.1 \times (-0.5) = 5.05$。$\lambda$ 增大，以增大对安全约束的重视。

**Uniqueness of the Bellman Equation Solution** Rewrite the Bellman equation in matrix-vector form:

$$\vec{v}_{\pi} = \vec{r}_{\pi} + \gamma p_{s}\vec{v}_{\pi} \implies (I - \gamma p_{s})\vec{v}_{\pi} = \vec{r}_{\pi}$$

where $p_s$ is the state transition probability matrix under policy $\pi$ ,
$$\vec{v}_{\pi} = (I - \gamma p_{s})^{-1}\vec{r}_{\pi}$$

 **$\epsilon$-Greedy Policy Probability Calculation**

$$\pi(a|s) = \begin{cases} 1 - \epsilon + \frac{\epsilon}{|A|} & \text{if } a = a^* (\text{greedy action}) \\ \frac{\epsilon}{|A|} & \text{if } a \ne a^* (\text{non-greedy}) \end{cases}$$
理论上建议的衰减速率为 $\epsilon_t \propto t^{-1/3}$（具体为 $\epsilon_t = t^{-1/3}(K \log t)^{1/3}$），这样可保证 $\mathbb{E}[\rho_T] \le \mathcal{O}(T^{2/3})$

Explore-First: $\mathbb{E}[\rho_T] \le \mathcal{O}(T^{2/3} (\log T)^{1/3})$ $\epsilon$-Greedy: $\mathbb{E}[\rho_T] \le \mathcal{O}(T^{2/3} (\log T)^{1/3})$ (当 $\epsilon_t$ 衰减时) UCB: $\mathbb{E}[\rho_T] \le \mathcal{O}(\sqrt{T \log T})$UCB 的理论遗憾界更低（更好），因为它随着 $T$ 的平方根增长，而前两者是 $T^{2/3}$。



**2. Fixed Point Iteration Convergence ($B^{\pi}$)**/Bellman Operator $T^{\pi}$ is a Contraction Mapping：

$$||B^{\pi}V-B^{\pi}V^{\prime}||_{\infty} = ||(r^{\pi}+\gamma T^{\pi}V) - (r^{\pi}+\gamma T^{\pi}V^{\prime})||_{\infty}$$

$$= ||\gamma(T^{\pi}V-T^{\pi}V^{\prime})||_{\infty}$$

$$= \gamma \max_{s} \left| \sum_{s^{\prime}}P(s^{\prime}|s,\pi(s)) (V(s^{\prime}) - V^{\prime}(s^{\prime})) \right|$$

$$\le \gamma \max_{s} \sum_{s^{\prime}}P(s^{\prime}|s,\pi(s)) \cdot ||V-V^{\prime}||_{\infty}$$

$$\le \gamma ||V-V^{\prime}||_{\infty} \quad (\because \sum P = 1, \gamma < 1)$$

$$
\therefore ||V_{t}^{\pi}-V^{\pi}||_{\infty} \le \gamma^{t} ||V_{0}^{\pi}-V^{\pi}||_{\infty} \rightarrow 0
$$

**3. Value Iteration Convergence ($B^{*}$)**

$$||B^{*}V-B^{*}V^{\prime}||_{\infty} = \max_{s} | \max_{a}Q(s,a) - \max_{a^{\prime}}Q^{\prime}(s,a^{\prime}) |$$

$$\le \max_{s} \max_{a} | Q(s,a) - Q^{\prime}(s,a) |$$

$$= \max_{s,a} \left| \left(r + \gamma\sum_{s^{\prime}}PV\right) - \left(r + \gamma\sum_{s^{\prime}}PV^{\prime}\right) \right|$$

$$\le \gamma \max_{s,a} \left| \sum_{s^{\prime}}P(s^{\prime}|s,a)(V(s^{\prime})-V^{\prime}(s^{\prime})) \right|$$

$$\le \gamma ||V-V^{\prime}||_{\infty}$$

**4. Explore-First Regret Analysis**

**Condition (Clean Event):**
$$\mu(a^{*}) - \mu(a) \le 2\sqrt{\frac{2\log(T)}{N}}$$
**Regret Bound:**
$$\rho_{T} \le N + (T-2N) \cdot 2\sqrt{\frac{2\log(T)}{N}}$$

$$\text{Set } N \approx (T/K)^{2/3}(\log T)^{1/3}:$$

$$\rho_{T} \le \mathcal{O}(T^{2/3}(\log T)^{1/3})$$

**5. UCB Regret Analysis**

**Condition (Clean Event):**
$$\mu(a_{t}) \ge \overline{\mu}_{t}(a_{t}) - \text{Bonus}_t$$

**Selection Rule:**
$$\overline{\mu}_{t}(a_{t}) + \text{Bonus}_t \ge \overline{\mu}_{t}(a^{*}) + \text{Bonus}_t \ge \mu(a^{*})$$

**Gap Bound:**
$$\Delta(a_{t}) = \mu(a^{*}) - \mu(a_{t}) \le 2\cdot\text{Bonus}_t = 2\sqrt{\frac{2\log(T)}{N_{a_t}}}$$
**Total Regret:**
$$\rho_{T} \le \sum_{t=1}^{T} \Delta(a_{t}) \le \sum_{a} \sum_{n=1}^{N_a} 2\sqrt{\frac{2\log T}{n}}$$

$$\le 4\sqrt{2\log T} \sqrt{K \sum N_a} \quad (\text{Jensen})$$

$$= \mathcal{O}(\sqrt{KT\log T})$$

**6. Importance Sampling (Off-Policy)**

**Ratio:**
$$\rho_{t} = \frac{\prod \pi(A_{k}|S_{k})P(S_{k+1}|S_{k},A_{k})}{\prod \mu(A_{k}|S_{k})P(S_{k+1}|S_{k},A_{k})} = \prod_{k=1}^{T-1}\frac{\pi(A_{k}|S_{k})}{\mu(A_{k}|S_{k})}$$

**Value Estimate:**
$$V(s) = \frac{\sum_{n}\rho_{t(s)}^{n}G_{t(s)}^{n}}{N(s)}$$

**7. TD-Learning as SGD**

**Loss:**
$$l(\theta) = \frac{1}{2}\left( V^{\pi}(s;\theta) - [r + \gamma V^{\pi}(s^{\prime};\theta^{*})] \right)^{2}$$

**Gradient:**
$$\frac{\partial l}{\partial \theta} = \delta(s,a,s^{\prime}) \cdot \frac{\partial V^{\pi}(s;\theta)}{\partial \theta} \approx \delta(s,a,s^{\prime})$$
**Update:**
$$\theta \leftarrow \theta - \alpha \delta$$

$$V(s) \leftarrow V(s) + \alpha (r + \gamma V(s^{\prime}) - V(s))$$
**Q-learning with linear function approximation**

Minimize the Mean Squared Error (MSE) between the current estimate and the TD target:
$$L(\theta) = \frac{1}{2} \left( y - Q(s,a;\theta) \right)^2$$

 **Target ($y$):** $r + \gamma \max_{a'} Q(s',a';\theta)$ (treated as a constant).

Calculate the gradient with respect to weights $\theta$:
$$\nabla_{\theta} L(\theta) = - \left( y - Q(s,a;\theta) \right) \nabla_{\theta} Q(s,a;\theta)$$

For **linear approximation** $Q(s,a;\theta) = \theta^{\top}\phi(s,a)$， the gradient is the feature vector:
$$\nabla_{\theta} Q(s,a;\theta) = \phi(s,a)$$

Apply SGD gradient descent $\theta \leftarrow \theta - \alpha \nabla_{\theta} L(\theta)$:

$$\theta \leftarrow \theta + \alpha \left( r + \gamma \max_{a'} Q(s',a';\theta) - \theta^{\top}\phi(s,a) \right) \phi(s,a)$$

1.In the agent-environment model, the state is updated by the environment, and the robot itself is usually modelled as part of the environment.
**###**
The 'Markov' aspect of a Markov decision process refers to the transition probabilities.
**###**
A reasonable alternative reward function for the pendulum environment is $r(s) = -\theta^2$.
**###**
Given a fixed policy, the value function can be computed via the inversion of a matrix or fixed point iteration.
**###**
The basis of policy and value iteration is the Bellman identity.
**###**
**Policy iteration** is the algorithm that converges to the true optimal value function.

2.Exploritation:Randomness and optimism
**###**
An expected cumulative regret **lower bounded** by $\mathcal{O}(T)$ implies that the learning algorithm does not asymptotically perform better than random guessing.
**###**
Epsilon-greedy and Boltzmann policies are algorithms that rely on **randomness** to ensure exploration.
**###**
To find the optimal policy in **model-based RL**, it is necessary to explore all state-action pairs.

3.When we **bootstrap**, we use a previous estimate instead of an unknown value.
**###**
**TD(0)** learning can be interpreted as a form of Stochastic Gradient Descent (SGD).
**###**
**Q-learning** asymptotically yields an optimal policy and requires $\mathcal{O}(|A||S|)$ memory.
**###**
RL algorithms are considered **on-policy** if they cannot reuse data generated using a different policy.
**###**
$R_{max}$, Monte-Carlo Control with importance sampling, and Q-learning are all **off-policy** algorithms.

7.Balancing safety and task execution is a challenge with ensuring safety through **reward shaping**.
**###**
**Lagrangian relaxations and trust-region** algorithms can be used to solve RL problems in CMDPs.
**###**
**Control barrier functions and constrained predictive control** are methods that can generally ensure safety during training.
**###**
**Control barrier functions** are positive only on the constraint/safe set.
**###**
**Model predictive control **iteratively solves optimal control problems online and suffers from high computational complexity.

8.A key requirement for policies in POMDPs is memory.
**###**
**Partial observability** in RL can be dealt with by inferring belief states, using a window of past observations, or using a recurrent neural network encoder.
**###**
It is wrong to state that reinforcement learning with believe states is generally tractable for large POMDPs. **True:**Believe states require a model of the transition probabilities.
Believe states require a model of the observation probabilities.
Reinforcement learning with believe states generally requires function approximation.
**###**
**Recurrent neural networks** can be used for RL in POMDPs to encode a finite sequence of observations, to approximately estimate a believe state, and by using their latent state as input to value function and policy parameterizations.
**###**
**Visuomotor policy training** can employ CNNs trained on off-distribution data.

9.**Inverse reinforcement learning** methods infer rewards.
**###**
There is a distribution shift in **behavior cloning** due to learning errors caused by limited data and limited expressivity of policy parameterization.
**###**
**DAgger** addresses the distribution shift problem in imitation learning by iterating between policy learning and data generation.
**###**
**Diffusion models** help address the challenge of multi-modality in imitation learning.

10.
**ppo trpo**: on policy
**###**
**TRPO implementation:** Constrain policy changes. Approximate loss function with first-order expansion
**###**
**PPO improve:** Replace second-order approximation with clipping function