**The Log-Trick** allows us to move the gradient operator inside an expectation. It is the mathematical foundation for Policy Gradient methods (like REINFORCE) in Reinforcement Learning.

$$\nabla_\theta p_\theta(x) = p_\theta(x) \nabla_\theta \log p_\theta(x)$$ **Derivation of the Gradient of Expectation:**We want to find the gradient of an expected value with respect to parameters $\theta$:

$$\begin{aligned} \nabla_\theta \mathbb{E}_{x \sim p_\theta}[f(x)] &= \nabla_\theta \int p_\theta(x) f(x) \, dx \\ &= \int \nabla_\theta p_\theta(x) f(x) \, dx \\ &= \int p_\theta(x) \nabla_\theta \log p_\theta(x) f(x) \, dx  \\ &= \mathbb{E}_{x \sim p_\theta} \left[ f(x) \nabla_\theta \log p_\theta(x) \right] \end{aligned}​$$

**Key Benefit:**It transforms the gradient of an integral (hard to compute) into an expectation that can be estimated via Monte Carlo sampling.

**Importance Sampling** is a technique to estimate properties (like the expected value) of a target distribution $p(x)$ while sampling from a different proposal distribution $q(x)$.**Derivation:**want to compute the expectation of $f(x)$ under $p(x)$, but we can only sample from $q(x)$.

$$\begin{aligned} \mathbb{E}_{x \sim p}[f(x)] &= \int f(x) p(x) \, dx \\ &= \int f(x) \frac{p(x)}{q(x)} q(x) \, dx  \\ &= \mathbb{E}_{x \sim q} \left[ f(x) \frac{p(x)}{q(x)} \right] \end{aligned}​$$

**The Importance Weight:**The term $\frac{p(x)}{q(x)}​$ is called the importance weight (or likelihood ratio), denoted as $w(x)​$.$$\mathbb{E}_{x \sim p}[f(x)] = \mathbb{E}_{x \sim q} [ f(x) w(x) ]​$$**Key Requirement:**The support of $q(x)​$ must cover the support of $p(x)​$ (i.e., $q(x) > 0​$ whenever $p(x) f(x) \neq 0​$). This technique reduces variance if $q(x)​$ is proportional to $|f(x)|p(x)​$

### 1.

An MDP is defined by the tuple:$\mathbb{S}$: Set of states.$\mathbb{A}$: Set of actions.$p(s_0)$: Initial state distribution.

$p(s'|s,a)$: Markov kernel / Transition probability.$$P(s'|s,a) = \mathbb{P}(S_{t+1}=s' | S_t=s, A_t=a)$$$r(s,a)$: Reward function.

**Matrix-Vector Notation**For finite states, the value function can be solved as a linear system:

$$V^{\pi} = r^{\pi} + \gamma T^{\pi}V^{\pi}​$$

$$V^{\pi} = (I - \gamma T^{\pi})^{-1}r^{\pi}​$$

**Fixed Point Iteration：**Iterative method to compute $V^\pi$ without matrix inversion:

$$V_{t}^{\pi} = r^{\pi} + \gamma T^{\pi}V_{t-1}^{\pi} =: \mathcal{B}^{\pi}V_{t-1}^{\pi}$$

**Convergence:** $||V_{t}^{\pi} - V^{\pi}||_{\infty} \le \gamma^{t}||V_{0}^{\pi} - V^{\pi}||_{\infty}​$

**Contraction Property**：The Bellman operator $\mathcal{B}^{\pi}$ is a contraction mapping:

$$||\mathcal{B}^{\pi}V - \mathcal{B}^{\pi}V^{\prime}||_{\infty} \le \gamma||V - V^{\prime}||_{\infty}$$

**Greedy Policy ($\pi_V$)**Selecting the best action given a value function $V$:

$$\pi_{V}(s) = \arg\max_{a} \left( r(s,a) + \gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)V(s^{\prime}) \right)$$

**Bellman Optimality Equation (Bellman's Identity)**The value function of the optimal policy ($V^*$) satisfies:

$$V^{*}(s) = \max_{a} \left( r(s,a) + \gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)V^{*}(s^{\prime}) \right)$$

**Policy Iteration：** Alternates between evaluating a policy and improving it.

**1.Policy Evaluation:** Compute $V^{\pi}$ (via matrix inverse or fixed point).

**2.Policy Improvement:** Update $\pi$ to be greedy w.r.t. $V^{\pi}$.

**Complexity:** Cubic per iteration $\mathcal{O}(|\mathbb{S}|^{3})$.

**Result:** Converges to the **exact** optimal policy.

**Value Iteration：**Iteratively applies the Bellman optimality operator. Update Rule:

$$V_{t}(s) = \max_{a} \left( r(s,a) + \gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)V_{t-1}(s^{\prime}) \right)$$

**Complexity:** Quadratic per iteration $\mathcal{O}(|\mathbb{S}|^{2}|\mathbb{A}|)​$.

**Result:** Converges to an **$\epsilon$-optimal** solution.

$$v_{\pi}(s) =\\ \sum_{a} \pi(a|s) \sum_{s',r} p(s', r|s, a) \left[ r + \gamma v_{\pi}(s') \right]\\, \quad \forall s \in \mathcal{S}​$$

$$q_{\pi}(s, a) =\\ \sum_{s',r} p(s', r|s, a) \left[ r + \gamma \sum_{a'} \pi(a'|s') q_{\pi}(s', a') \right]​$$

$$v_{*}(s) = \max_{a} \sum_{s',r} p(s', r|s, a) \left[ r + \gamma v_{*}(s') \right]​$$

$$q_{*}(s, a) =\\ \sum_{s',r} p(s', r|s, a) \left[ r + \gamma \max_{a'} q_{*}(s', a') \right]​$$

### 2.

**Markov Property:** The future depends only on the current state and action, not the history.

$$\pi_{greedy}(s) = \arg\max_{a} \left( r(s,a) + \gamma \sum_{s'} P(s'|s,a)V(s') \right)$$

**Multi-Armed Bandits (MAB)**  **Problem:** Sequential selection of arms $a_t$ to maximize cumulative reward $R_t$ from unknown distributions $p_{a_t}$. **Exploration-Exploitation Conflict:** Balancing gathering information (exploration) vs. maximizing immediate reward (exploitation).

**Cumulative Regret** ($\rho_T$): Difference between expected reward of optimal arm and expected reward of chosen arms.

$$\rho_{T} = \max_{a}\sum_{t=1}^{T}\mathbb{E}[r_{a}] - \sum_{t=1}^{T}\mathbb{E}[r_{a_{t}}]$$

**1.Explore-First**Strategy: Explore each arm $N_a$ times, then exploit the best average.

**Hoeffding Inequality (Bound):**

$$P\left(|\mu(a) - \bar{\mu}(a)| \le \sqrt{\frac{2 \log(T)}{N_a}}\right) \ge 1 - \frac{2}{T^4}$$

Regret Bound: $\mathbb{E}[\rho_{T}] \le \mathcal{O}(KT^{2/3}(\log(T))^{1/3})$.

**2.Epsilon-Greedy ($\epsilon$-Greedy)**Strategy: With probability $\epsilon$, explore random arm; else exploit best estimated arm.

Incremental Update Rule:

$$\bar{\mu}_{N_{a}+1}(a) = \bar{\mu}_{N_{a}}(a) + \frac{1}{N_{a}}(R_{N_{a}} - \bar{\mu}_{N_{a}}(a))$$

General Step Size Update:

$$\text{NewEst} = \text{OldEst} + \text{StepSize} \times (\text{Target} - \text{OldEst})$$

**Robbins-Monro Convergence Conditions:** $\sum \alpha_N = \infty$ and $\sum \alpha_N^2 < \infty$.

**3.Upper Confidence Bound (UCB)**Strategy: Optimism in the face of uncertainty. Select arm maximizing the UCB. Selection Formula:

$$a_t = \arg\max_a \left( \bar{\mu}(a) + c \sqrt{\frac{\log(T)}{N_a}} \right)$$

Regret Bound: $\mathbb{E}[\rho_{T}] \le \mathcal{O}(\sqrt{KT \log(T)})$.

**4.Gradient Bandits (Random Policies)**Strategy: Learn preferences $H_t(a)$ using Softmax (Boltzmann) distribution.

Policy Probability:$$\pi_{t}(a) = \frac{e^{H_{t}(a)}}{\sum_{k=1}^{K}e^{H_{t}(k)}}$$

Stochastic Gradient Ascent Update (with baseline $\bar{R}_t$):

$$H_{t+1}(a) = H_{t}(a) + \alpha(R_{t} - \bar{R}_{t})(\mathbb{I}_{A_{t}=a} - \pi_{t}(a))$$

**Learning the Model：** Transition Probability Estimate (Counting):

$$\hat{P}(s'|s,a) = \frac{N(s',s,a)}{\sum_{s''} N(s'',s,a)}​$$

Reward Function Estimate:$$\hat{r}(s,a) = \frac{\sum R_t}{N(s,a)}​$$

**$\epsilon$-Greedy in MDPs:** Acts randomly with prob $\epsilon$, otherwise greedy w.r.t. learned model.

**$R_{max}$ Algorithm (Optimism):**

Initializes unknown rewards to $R_{max}$ (upper bound). Unknown transitions point to a hypothetical "fairy tale" state $S^*$ with maximum reward. **Implicit exploration:** The agent is attracted to unknown states because it assumes they yield high rewards.

### 3.

**Exploration-Exploitation Conflict:** Balancing trying new arms to find better rewards (exploration) vs. playing the best-known arm (exploitation).

**Random Policies (Boltzmann):** Sample actions based on a learned preference distribution.

**Regret Bound:**

$$\rho_{T} \ge T\frac{(K-1)}{K} \min_{a \ne a'} \mathbb{E}[r_{a} - r_{a'}]​$$

Empirical Mean:$$\mu(a) \approx \frac{\sum_{t=1}^{N_{a}}R_{t}}{N_{a}} = \overline{\mu}(a)$$

Hoeffding Inequality:

$$p(|\mu(a) - \overline{\mu}(a)| \le \sqrt{\frac{2 \log(T)}{N_{a}}}) \ge 1 - \frac{2}{T^{4}}​$$

Epsilon-Greedy Decay Rate:

$$\epsilon_{t} = t^{-\frac{1}{3}}(K \log(t))^{1/3}$$

Upper Confidence Bound (UCB):

$$ucb(a) = \overline{\mu}(a) + \sqrt{\frac{2 \log(T)}{N_{a}}}$$

Boltzmann Distribution:$$\pi_{t}(a) = \frac{e^{H_{t}(a)}}{\sum_{k=1}^{K}e^{H_{t}(k)}}$$

Gradient Ascent for Policy Preferences:

$$H_{t+1}(a) = H_{t}(a) + \alpha(R_{t} - \overline{R}_{t})(\mathbb{I}_{A_{t}=a} - \pi_{t}(a))$$

**Model-Based Reinforcement Learning**

Empirical Transition Probability:

$$\hat{P}(s'|s,a) = \frac{N(s',s,a)}{\sum_{s'}N(s',s,a)}$$

Empirical Reward Function:

$$\hat{r}(s,a) = \frac{\sum_{a_{t}=A, s_{t}=S} R_{t}}{\sum_{S'}N(S',s,a)}$$

**Temporal Difference (TD) Learning**TD(0) Update Rule:

$$V(S_{t}) \leftarrow V(S_{t}) + \alpha_{t}(R_{t} + \gamma V(S_{t+1}) - V(S_{t}))$$

**SARSA:** State-Action-Reward-State-Action. **On-Policy:** Learns the value of the policy currently being followed (including exploration steps).

**SARSA Update:**

$$Q(S_{t},A_{t}) \leftarrow Q(S_{t},A_{t}) + \alpha_{t}(R_{t} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_{t},A_{t}))$$

**Q-Learning:** Uses the greedy action in the update target ($max_{a'}$).

**Off-Policy:** Learns the optimal policy $Q^*$ while following a different behavior policy (e.g., $\epsilon$-greedy).

**Q-Learning Update:**

$$Q(S_{t},A_{t}) \leftarrow Q(S_{t},A_{t}) + \alpha_{t}(R_{t} + \gamma \max_{a'} Q(S_{t+1},a') - Q(S_{t},A_{t}))$$

**Function Approximation:** Replaces tabular values with a parameterized function $V(s;\theta)$ (e.g., Neural Network) to handle large or continuous state spaces.

**DQN Heuristics**：**1.Experience Replay:** Store transitions in a buffer and sample minibatches to break correlation.**Target Network:** Use a frozen copy of parameters ($\theta^{old}$) for the target value to stabilize training.

**TD Update as SGD:**

$$V^{\pi}(s;\theta) \leftarrow V^{\pi}(s;\theta) + \alpha_{t}(r + \gamma V^{\pi}(s';\theta^{old}) - V^{\pi}(s;\theta))$$

**Q-Learning Loss Function:**

$$l(\theta;s,a) = \frac{1}{2}(r(s,a) + \gamma \max_{a'} Q^{*}(s',a';\theta^{old}) - Q^{*}(s,a;\theta))^2$$

**DQN Gradient Update:**

$$\theta \leftarrow \theta + \alpha_{t}\delta_{B}(s,a,s')\nabla_{\theta}Q^{*}(s,a;\theta)$$(Where $\delta_{B}$ is the Bellman error)

### 7.

**A. Reward Shaping**Define a reward function that assigns large negative values to unsafe behavior.

Formula:$$r(s,a) = r_{task}(s,a) + r_{safety}(s,a)​$$

**B. Risk-sensitive RL**Optimizes for risk tails (low-probability events) rather than just expected value.**Pros/Cons:** Easy to apply standard RL; but has no strong guarantees and requires complicated reward design. **Risk Objective:**

$$V^{\pi}(s) = \mathbb{R}_{\beta}^{*}[\sum_{k=0}^{\infty}\gamma^{k}r(s_{k},\pi(s_{k}))]​$$

**Metrics:** Conditional Value at Risk (CVaR), Entropic risk.

**Constrained MDPs (CMDP)**Explicitly represents safety conditions as constraints.

Policy Optimization Problem:

Maximize $\pi(s) = \arg\max_{a}(r_{task}(s,a) + \gamma\mathbb{E}_{s^{\prime}}[V_{task}^{\pi}(s^{\prime})])$  Subject to:

$$r_{safety}(s,a) + \gamma\mathbb{E}_{s^{\prime}}[V_{safety}^{\pi}(s^{\prime})] \ge c$$

Safety Value Function:

$$V_{safety}^{\pi}(s) = r_{safety}(s,a) + \gamma\mathbb{E}_{s^{\prime}}[V_{safety}^{\pi}(s^{\prime})]$$

**D. Constrained Policy Optimization (CPO)**Trust-region optimization for CMDPs.

Formulation:Maximize $\mathbb{E} [A_{task}(s, a)]$ Subject to:

1.Safety: $\mathbb{E}[V_{safety}^{\pi}(s_{0})] - \frac{1}{1-\gamma}\mathbb{E}[A_{safety}^{\pi}(s,a)] \ge c$

2.Trust Region (KL-divergence): $\overline{D}_{KL}(\pi||\pi_{k}) \le \delta$

**E. Primal-Dual Method (Lagrangian Relaxation)** Solves the CMDP by converting it to a dual problem (min-max).

Lagrangian:

$$d(\lambda) = \max_{\pi}L(\pi,\lambda) = \max_{\pi}\mathbb{E}[V_{task}(s,a)] + \lambda(\mathbb{E}[V_{safety}(s,a)] - c)$$

Optimization:$$\min_{\lambda\ge0}d(\lambda)$$

Gradient descent for $\lambda$. Gradient ascent for $\pi$.

**Control Barrier Functions (CBF)**  A function $h(x)$ that is positive inside the constraint set, 0 at the boundary, and negative outside. Used as a safety filter/shield.

Safety Condition:$$\mathbb{E}[h(s^{\prime})] \ge \alpha h(s)$$  where $\alpha \in [0,1]​$.

Safety Filter (Optimization):

$$\pi_{safe}(s) = \min_{a} ||a - \pi(s)||^{2}$$

Subject to: $\mathbb{E}[h(s^{\prime})] \ge \alpha h(s)$

**Learning:** CBFs can be learned from expert demonstrations or RL policy roll-outs.

**Constrained Predictive Control (MPC)**Online optimization over a sequence of actions (Model Predictive Control). Ensures strict constraint satisfaction.

Formula:$$\min_{a_{0},...,a_{K}}\mathbb{E}[\sum_{k=0}^{K}\gamma^{k}r(s_{k},a_{k}) + V(s_{K+1})]$$

Subject to: $s_{k} \in \mathbb{S}_{safe}, a_{k} \in \mathbb{A}_{safe}$

**Solvers:** Constrained cross-entropy method, Sequential quadratic programming, Non-convex collocation.

**Lagrangian Update Rule**

$$L(\pi, \lambda) = J_r(\pi) - \lambda (J_c(\pi) - d_{limit})$$

$$\lambda_{k+1} = \max(0, \lambda_k + \eta (J_c(\pi_k) - d_{limit}))$$

### 8.

**Partial Observability:** The agent does not have full information about the state ($s$); it only receives observations ($o$). Single observations are often insufficient for decision-making.

**POMDP (Partially Observable MDP):** An MDP augmented with a set of observations and observation probabilities.  **Belief State ($b_k$):** A probability distribution over possible states given the history of actions and observations. It acts as a sufficient statistic for the history.**Planning with Belief States:** Using Bayesian updates to estimate states (similar to Kalman filtering).

**Maximum Likelihood Estimation (MLE) Approximation:** A simplification where the agent acts based on the most likely state in the belief distribution rather than the full distribution.

Belief State:$$b_{k}=p(s_{k}|o_{0},a_{0},...,a_{k-1},o_{k})$$

Belief State Update:$$b_{k}=g(b_{k-1},a_{k-1},o_{k})​$$

**POMDP Objective (Fully Observable vs. Partially Observable):**

Fully Observable: $\mathbb{E}[\sum_{k=0}^{\infty}\gamma^{k}r(s_{k},a_{k})]$

Partially Observable (Belief-based):

$$\mathbb{E}[\sum_{k=0}^{\infty}\sum_{s\in S}\gamma^{k}b_{k}(s)r(s_{k},a_{k})]$$

Constrained Predictive Control (MPC approach):

$$\min_{a_{0},...,a_{K}}\mathbb{E}[\sum_{k=0}^{K}\gamma^{k}r(s_{k},a_{k})+V(s_{K+1})]$$

**Visuomotor Learning:** RL where observations are high-dimensional images.

### 9.

 **1. Inverse Reinforcement Learning (IRL)**

 **Goal:** Infer the unknown reward function using expert demonstration data, assuming the expert is optimal.**Challenges:** Reward ambiguity (trivial solution where reward is 0), dependence on expert optimality, and computational complexity.**Feature Matching:** If a policy matches the expert's expected feature counts, their value is close to the expert's value. **Max-Margin IRL:** Finds a reward function that maximizes the margin between the expert policy and other policies.
 **Max-Entropy IRL:** Parameterizes path probabilities and maximizes entropy to resolve reward ambiguity (using a Boltzmann distribution over paths).

**IRL Assumption** (Expert Optimality):

$$\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t})|\pi^{*}]\ge\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t})|\pi] \quad \forall\pi​$$

Linear Reward Parameterization: $$r(s)=w^{T}\phi(s)$$

**Max-Margin Optimization:**

$$min_{w,\xi}||w||^{2}+c\xi$$  Subject to:

$${w^{*}}^{T}\mu(\pi^{*})\ge{w^{*}}^{T}\mu(\pi)+m(\pi,\pi^{*})-\xi​$$

Feature Expectation Matching Bound:

$$||\mu(\pi)-\mu(\pi^{*})||_{1} \le \epsilon \implies |{w^{*}}^{T}\mu(\pi)-{w^{*}}^{T}\mu(\pi^{*})|\le\epsilon$$

**Max-Entropy Objective:**

$$max_{P}-\sum_{\zeta}P(\zeta)log~P(\zeta)$$

Subject to:$$\sum_{\zeta}P(\zeta)\mu(\zeta)=\mu(\pi^{*})$$

Max-Entropy Distribution Solution:

$$P(\zeta)=\frac{1}{Z(\theta)}exp(w^{T}\mu(\zeta))​$$

**2. Imitation Learning (Behavior Cloning)**

 **Behavior Cloning (BC):** Frames policy learning as a supervised regression/classification problem using a fixed dataset of expert trajectories.
 **Loss Functions:** Typically negative log-likelihood or square loss.
 **Distribution Shift:** The primary failure mode of BC. The training distribution comes from the expert ($p_{\pi^*}$), but the test distribution comes from the trained policy ($p_{\hat{\pi}^*}$).
 **Compounding Errors:** Small errors accumulate, leading the agent into states not seen during training.

**Formulas:** BC Optimization:

  $$\hat{\pi}^{*}=arg~min_{\pi}\sum_{i=1}^{N}l(\pi,s_{i},a_{i})$$

 BC Approximation: $$\approx arg~min\mathbb{E}_{s\sim p_{\pi^{*}}}[l(\pi,s,\pi^{*}(s))]$$

 Compounding Error Bound:  Returns drop by at most $\epsilon T^{2}$

**3. On-Policy Imitation Learning (DAgger)**

 **DAgger (Dataset Aggregation):** An iterative algorithm to address distribution shift.
 **Process:** 1 Train policy on current data.
  2 Rollout a mixture of the expert and learned policy.
  3 Query the expert to label new states.
  4 Aggregate data and retrain.
 **Result:** Reduces the error bound significantly compared to standard behavior cloning.

Policy Mixing:$$\pi_{i}=\beta_{i}\pi^{*}+(1-\beta_{i})\hat{\pi}$$

**Data Aggregation**:$$\mathcal{D}\leftarrow\mathcal{D}\cup\mathcal{D}_{i}$$

DAgger Error Bound: Returns drop by at most $\epsilon T$ (linear in time horizon).

**4.Diffusion Models in Robotics**

**Multi-modality:** Standard policies (e.g., Gaussian) struggle with multi-modal behavior; generative models solve this.**Diffusion Process:** Interprets imitation learning as fitting a probability distribution. It models trajectory generation as a reverse denoising process from Gaussian noise.**Planning:** Diffusion can be used for planning by conditioning on goals or initial states (inpainting).

Reverse (Denoising) Process:

$$p_{\theta}(\tau_{0:K}) = p(\tau_K) \prod_{k=1}^K p_{\theta}(\tau_{k-1}|\tau_k)$$

Modeled as transitions like: $\mathcal{N}(\mu_{\theta}(\tau_{K}),\Sigma_{K}) \to \dots \to \mathcal{N}(\mu_{\theta}(\tau_{1}),\Sigma_{1})$

Forward (Diffusion) Step:

$$q(\tau_k|\tau_{k-1}) = \mathcal{N}(\sqrt{1-\beta_{k}}\tau_{k-1}, \beta_{k}I)$$

### 10.

**1. Q-Learning** **Pros:** Simple, stable **Cons:** Discrete only

**2. REINFORCE** **Pros:** Works in continuous spaces **Cons:** High variance

**3. Actor-Critic** **Pros:** Lower variance, more efficient **Cons:** Still unstable

**Deep Q-Learning (DQN)**
Experience Replay ,Target Network

DQN Loss Function (MSE):

$$l(\theta;s,a) = \frac{1}{2} \left( r + \gamma \max_{a'} Q(s', a'; \theta^{old}) - Q(s,a;\theta) \right)^2$$

*Target:* $r + \gamma \max_{a'} Q(s', a'; \theta^{old})$

**Policy-Based Methods (REINFORCE)**

Objective Function: $$J(\theta) = \mathbb{E}_{\pi_{\theta}}[\sum_{t} \gamma^{t} r_{t}]$$

**Policy Gradient Theorem:**

$$\nabla_{\theta}J(\theta) = \mathbb{E}_{\pi_{\theta}}[\nabla_{\theta} \log \pi_{\theta}(a|s) Q^{\pi_{\theta}}(s,a)]$$
在 REINFORCE 中使用 $G_t$ 作为 $Q(s,a)$ 的无偏估计，但方差很大。引入 Critic 估计 $Q(s,a)$ 或优势函数 $A(s,a) = Q(s,a) - V(s)$ 可以显著降低方差

REINFORCE Update Rule (using Return $G_t$ instead of $Q$):

$$\nabla_{\theta}J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_{t}|s_{t}) G_{t}$$

$$\theta \leftarrow \theta + \alpha \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) G_t$$

**Actor-Critic Methods**

Actor-Critic Gradient:

$$\nabla_{\theta}J(\theta) = \mathbb{E}_{\pi_{\theta}}[\nabla_{\theta} \log \pi_{\theta}(a|s) A^{\pi_{\theta}}(s,a)]$$

Advantage Function ($A$): Represents how much better a specific action is compared to the average value of the state.

$$A(s,a) = Q(s,a) - V(s)$$

Baseline Property: Subtracting a state-dependent baseline $b(s)$ (like $V(s)$) does not bias the gradient but reduces variance:

$$\mathbb{E}_{\pi_{\theta}}[\nabla_{\theta} \log \pi_{\theta}(a|s) b(s)] = 0$$

### 11.

**Soft Actor-Critic (SAC)**

**Concept:** Off-policy, entropy-regularized RL. Adds an entropy bonus to the objective to encourage exploration and prevent premature convergence.

Entropy-Regularized Objective:

$$J(\theta)=E_{\pi_{\theta}}[\Sigma_{t}\gamma^{t}(r_{t}+\alpha\mathcal{H}(\pi(a|s_{t})))]$$

Entropy: $$\mathcal{H}(\pi(a|s))=-E[log~\pi(a|s)]$$

Soft Bellman Equation (Value):

$$V^{\pi}(s)=E_{a\sim\pi}[Q^{\pi}(s,a)-\alpha log~\pi(a|s)]$$

**Key Features:** Stochastic Gaussian actor, uses Replay Buffer, $\alpha$ (temperature) controls exploration.

**Deep Deterministic Policy Gradient (DDPG)**

**Concept:** Off-policy method for continuous control using a deterministic actor.

Update Rule: Learns via gradients from a critic.

$$\nabla_{\theta}J(\theta)=E_{s\sim D}[\nabla_{a}Q_{\phi}(s,a)\nabla_{\theta}\pi_{\theta}(s)]$$

**Key Features:** Deterministic actor ($a=\pi_{\theta}(s)​$), requires noise injection for exploration, uses Replay Buffer and Target Networks.

**Trust Region Policy Optimization (TRPO)**

**Concept:** On-policy method that constrains policy updates to a "Trust Region" to ensure monotonic improvement and stability.**Objective:** Maximize surrogate objective subject to a "Hard" KL-divergence constraint.

*Maximize:* $$E_{s,a\sim\pi_{old}}[\frac{\pi_{\theta}(a|s)}{\pi_{old}(a|s)}A^{\pi_{old}}(s,a)]​$$

*Subject to:* 

$$E_{s\sim\pi_{old}}[D_{KL}(\pi_{old}(\cdot|s)||\pi_{\theta}(\cdot|s))]\le\delta​$$

**Importance Sampling:** Used to estimate the new policy's performance using old data (probability ratio).

Update Rule (Natural Gradient): Solved via conjugate gradient and Hessian ($H$) of the KL divergence.

$$\theta_{k+1}=\theta_{k}+\sqrt{\frac{2\delta}{g^{T}H^{-1}g}}H^{-1}g​$$

(where $g​$ is the policy gradient and $H​$ is the Fisher Information Matrix)

**Proximal Policy Optimization (PPO)**

**Concept:** A lightweight, first-order approximation of TRPO. Replaces the complex KL constraint/Hessian with a simple clipping mechanism ("Soft" constraint).

Probability Ratio:$$r_{t}(\theta)=\frac{\pi_{\theta}(a|s)}{\pi_{old}(a|s)}$$

Clipped Objective: Prevents the ratio $r_t​$ from deviating too far from 1.

$$L^{CLIP}(\theta)=E[min(r_{t}(\theta)A_{t}, clip(r_{t}(\theta), 1 - \epsilon , 1 + \epsilon )A_t)]$$

**Total Loss Function:**

$$L_{Total}(\theta)=L^{CLIP}(\theta)+c_{1}L^{VF}-c_{2}\mathcal{H}(\pi)​$$

(Includes Value Function loss $L^{VF}$ and Entropy bonus $\mathcal{H}$)

**Generalized Advantage Estimation (GAE)**

**Concept:** A technique used (commonly with PPO) to balance bias and variance in advantage estimation.

TD Error:$$\delta_{t}=r_{t}+\gamma~V(S_{t+1})-V(S_{t})​$$

GAE Formula: Exponentially weighted average of TD errors.

$$\hat{A_{t}}=\Sigma_{l=0}^{\infty}(\gamma\lambda)^{l}\delta_{t+l}​$$  (Parameter $\lambda​$ controls the bias-variance trade-off)

 **Limitations of PPO & Modern Challenges** ：**On-policy Inefficiency:** Discards samples after one update; poor sample efficiency compared to off-policy methods (like SAC).**Clipping Heuristic:** The clipped ratio is a loose approximation of the KL constraint; does not guarantee true safety.**Hyperparameter Sensitivity:** Performance varies heavily based on clipping range ($\epsilon​$), learning rate, and batch size.**Value Function Lag:** The critic often learns slower than the actor, leading to stale advantage estimates.**Limited Exploration:** Relies on random noise; struggles in sparse-reward environments without intrinsic curiosity.

**Bellman Operator $T^{\pi}$ is a Contraction Mapping：**

$$|T^{\pi}v_{1}(s) - T^{\pi}v_{2}(s)|​$$

$$= |\sum_{s',r,a}p(s',r|s,a)\pi(a|s)[r+\gamma v_{1}(s')] - \sum_{s',r,a}p(s',r|s,a)\pi(a|s)[r+\gamma v_{2}(s')]|​$$

$$\le \gamma \sum_{s',r,a}p(s',r|s,a)\pi(a|s)|v_{1}(s') - v_{2}(s')|​$$

$$\le \gamma \sum_{s',r,a}p(s',r|s,a)\pi(a|s)||v_{1} - v_{2}||_{\infty}​$$

$$= \gamma ||v_{1} - v_{2}||_{\infty}$$

推导策略 $\pi$ 下的状态价值函数 $V^\pi(s)$ 的贝尔曼方程（递归形式）

价值函数定义为：$V^\pi(s) = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r(S_t, \pi(S_t)) | S_0 = s]$ 。

$$\begin{aligned} V^\pi(s) &= \mathbb{E}[r(s, \pi(s)) + \sum_{t=1}^{\infty} \gamma^t r(S_t, \pi(S_t)) | S_0 = s] \\ &= r(s, \pi(s)) + \gamma \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k r(S_{k+1}, \pi(S_{k+1})) | S_0 = s] \quad (\text{令 } k=t-1) \\ &= r(s, \pi(s)) + \gamma \sum_{s'} p(s'|s, \pi(s)) \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k r(S_{k+1}, \pi(S_{k+1})) | S_1 = s'] \\ &= r(s, \pi(s)) + \gamma \sum_{s'} p(s'|s, \pi(s)) V^\pi(s') \end{aligned}$$


策略迭代：通常在较少的迭代次数内收敛到精确解，但每一步需要进行策略评估（求解线性方程组或多次迭代），单步计算代价为 $O(|\mathcal{S}|^3)$ 。适用于状态空间较小的情况。价值迭代：每一步计算量较小 $O(|\mathcal{S}|^2|\mathcal{A}|)$，需要更多次迭代才能收敛到 $\epsilon$-最优解。

Proof:Expectation of $h(s')$: Let $s' = \mu + \epsilon$ where $\epsilon \sim \mathcal{N}(0, 0.1I)$ and $s \in \mathbb{R}^2$.

$$\mathbb{E}[h(s')] = 1 - \mathbb{E}[(\mu+\epsilon)^\top(\mu+\epsilon)] = 1 - \|\mu\|^2 - \text{Tr}(\text{Cov}(\epsilon))$$

$$\mathbb{E}[h(s')] = 1 - \|\mu\|^2 - 0.2 \quad (\text{since } \text{Tr}(0.1I_{2\times2}) = 0.2)$$

Calculate Mean $\mu$: Given $a = -[0.75, 1.0]s$, $A = [[0, 0.5], [0.75, 1]]$, $B = [0, 1]^\top$.$$\mu = As + Ba = \begin{bmatrix} 0.5s_2 \\ 0.75s_1 + s_2 \end{bmatrix} + \begin{bmatrix} 0 \\ -0.75s_1 - s_2 \end{bmatrix} = \begin{bmatrix} 0.5s_2 \\ 0 \end{bmatrix}$$Verify Inequality: We need to show $0.8 - (0.5s_2)^2 \ge 0.25(1 - s_1^2 - s_2^2)$.$$0.8 - 0.25s_2^2 \ge 0.25 - 0.25s_1^2 - 0.25s_2^2$$

$$0.55 + 0.25s_1^2 \ge 0$$This holds true for any real $s_1$. Thus, the CBF condition is satisfied.

**Safety using Lagrangian Relaxations** 使用 Primal-Dual 方法解决 CMDP。

 $\lambda$ 范围：Lagrange 乘子 $\lambda$ 用于将约束违规转化为惩罚。根据 KKT 条件或对偶问题定义，$\lambda$ 必须是非负的，即 $\lambda \ge 0$。
**$\lambda$ 更新：**通常约束形式为 $V_{safety} \ge c$ (安全值要高) 或 $Cost \le c$ (代价要低)。题目中 $V_{safety}=0.5 < c=1.0$。假设约束是 $V_{safety} \ge c$，则当前违规。对偶更新（Gradient Ascent on Dual）：$\lambda \leftarrow \lambda - \eta \nabla_\lambda L$。
Lagrangian $L = V_{task} + \lambda (V_{safety} - c)$。
$\lambda_{new} = \lambda - \eta (V_{safety} - c) = 5 - 0.1 \times (0.5 - 1.0) = 5 - 0.1 \times (-0.5) = 5.05$。$\lambda$ 增大，以增大对安全约束的重视。

**Uniqueness of the Bellman Equation Solution** Rewrite the Bellman equation in matrix-vector form:

$$\vec{v}_{\pi} = \vec{r}_{\pi} + \gamma p_{s}\vec{v}_{\pi} \implies (I - \gamma p_{s})\vec{v}_{\pi} = \vec{r}_{\pi}$$

where $p_s$ is the state transition probability matrix under policy $\pi$ 

$$\vec{v}_{\pi} = (I - \gamma p_{s})^{-1}\vec{r}_{\pi}$$

 **$\epsilon$-Greedy Policy Probability Calculation**

$$\pi(a|s) = \begin{cases} 1 - \epsilon + \frac{\epsilon}{|A|} & \text{if } a = a^* (\text{greedy action}) \\ \frac{\epsilon}{|A|} & \text{if } a \ne a^* (\text{non-greedy}) \end{cases}$$
理论上建议的衰减速率为 $\epsilon_t \propto t^{-1/3}$（具体为 $\epsilon_t = t^{-1/3}(K \log t)^{1/3}$），这样可保证 $\mathbb{E}[\rho_T] \le \mathcal{O}(T^{2/3})$

Explore-First: $\mathbb{E}[\rho_T] \le \mathcal{O}(T^{2/3} (\log T)^{1/3})$ $\epsilon$-Greedy: $\mathbb{E}[\rho_T] \le \mathcal{O}(T^{2/3} (\log T)^{1/3})$ (当 $\epsilon_t$ 衰减时) UCB: $\mathbb{E}[\rho_T] \le \mathcal{O}(\sqrt{T \log T})$UCB 的理论遗憾界更低（更好），因为它随着 $T$ 的平方根增长，而前两者是 $T^{2/3}$。

**1. Recursive Definition of Value Functions**

$$
\begin{aligned}
V^{\pi}(s) &= \mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}r(S_{t},\pi(S_{t})) \mid S_{0}=s\right] \\
&= \mathbb{E}[\gamma^{0}r(S_{0},\pi(S_{0})) \mid S_{0}=s] + \mathbb{E}\left[\sum_{t=1}^{\infty}\gamma^{t}r(S_{t},\pi(S_{t})) \mid S_{0}=s\right] \\
&= r(s,\pi(s)) + \mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t+1}r(S_{t+1},\pi(S_{t+1})) \mid S_{0}=s\right] \\
&= r(s,\pi(s)) + \gamma \mathbb{E}_{s^{\prime}\sim P(\cdot|s)}\left[\mathbb{E}\left[\sum_{k=0}^{\infty}\gamma^{k}r(S_{k+1},\pi(S_{k+1})) \mid S_{1}=s^{\prime}\right] \mid S_{0}=s\right] \\
&= r(s,\pi(s)) + \gamma \sum_{s^{\prime}}P(s^{\prime}|s,\pi(s))V^{\pi}(s^{\prime})
\end{aligned}
$$

**2. Fixed Point Iteration Convergence ($B^{\pi}$)**

$$
\begin{aligned}
||B^{\pi}V-B^{\pi}V^{\prime}||_{\infty} &= ||(r^{\pi}+\gamma T^{\pi}V) - (r^{\pi}+\gamma T^{\pi}V^{\prime})||_{\infty} \\
&= ||\gamma(T^{\pi}V-T^{\pi}V^{\prime})||_{\infty} \\
&= \gamma \max_{s} \left| \sum_{s^{\prime}}P(s^{\prime}|s,\pi(s)) (V(s^{\prime}) - V^{\prime}(s^{\prime})) \right| \\
&\le \gamma \max_{s} \sum_{s^{\prime}}P(s^{\prime}|s,\pi(s)) \cdot ||V-V^{\prime}||_{\infty} \\
&\le \gamma ||V-V^{\prime}||_{\infty} \quad (\because \sum P = 1, \gamma < 1)
\end{aligned}
$$

$$
\therefore ||V_{t}^{\pi}-V^{\pi}||_{\infty} \le \gamma^{t} ||V_{0}^{\pi}-V^{\pi}||_{\infty} \rightarrow 0
$$

**3. Value Iteration Convergence ($B^{*}$)**

$$
\begin{aligned}
||B^{*}V-B^{*}V^{\prime}||_{\infty} &= \max_{s} | \max_{a}Q(s,a) - \max_{a^{\prime}}Q^{\prime}(s,a^{\prime}) | \\
&\le \max_{s} \max_{a} | Q(s,a) - Q^{\prime}(s,a) | \\
&= \max_{s,a} \left| \left(r + \gamma\sum_{s^{\prime}}PV\right) - \left(r + \gamma\sum_{s^{\prime}}PV^{\prime}\right) \right| \\
&\le \gamma \max_{s,a} \left| \sum_{s^{\prime}}P(s^{\prime}|s,a)(V(s^{\prime})-V^{\prime}(s^{\prime})) \right| \\
&\le \gamma ||V-V^{\prime}||_{\infty}
\end{aligned}
$$

**4. Explore-First Regret Analysis**

**Condition (Clean Event):**
$$\mu(a^{*}) - \mu(a) \le 2\sqrt{\frac{2\log(T)}{N}}$$

**Regret Bound:**
$$
\begin{aligned}
\rho_{T} &\le N + (T-2N) \cdot 2\sqrt{\frac{2\log(T)}{N}} \\
&\text{Set } N \approx (T/K)^{2/3}(\log T)^{1/3}: \\
\rho_{T} &\le \mathcal{O}(T^{2/3}(\log T)^{1/3})
\end{aligned}
$$

**5. UCB Regret Analysis**

**Condition (Clean Event):**
$$\mu(a_{t}) \ge \overline{\mu}_{t}(a_{t}) - \text{Bonus}_t$$

**Selection Rule:**
$$\overline{\mu}_{t}(a_{t}) + \text{Bonus}_t \ge \overline{\mu}_{t}(a^{*}) + \text{Bonus}_t \ge \mu(a^{*})$$

**Gap Bound:**
$$\Delta(a_{t}) = \mu(a^{*}) - \mu(a_{t}) \le 2\cdot\text{Bonus}_t = 2\sqrt{\frac{2\log(T)}{N_{a_t}}}$$
**Total Regret:**
$$
\begin{aligned}
\rho_{T} \le \sum_{t=1}^{T} \Delta(a_{t}) &\le \sum_{a} \sum_{n=1}^{N_a} 2\sqrt{\frac{2\log T}{n}} \\
&\le 4\sqrt{2\log T} \sqrt{K \sum N_a} \quad (\text{Jensen}) \\
&= \mathcal{O}(\sqrt{KT\log T})
\end{aligned}
$$

**6. Importance Sampling (Off-Policy)**

**Ratio:**
$$\rho_{t} = \frac{\prod \pi(A_{k}|S_{k})P(S_{k+1}|S_{k},A_{k})}{\prod \mu(A_{k}|S_{k})P(S_{k+1}|S_{k},A_{k})} = \prod_{k=1}^{T-1}\frac{\pi(A_{k}|S_{k})}{\mu(A_{k}|S_{k})}$$

**Value Estimate:**
$$V(s) = \frac{\sum_{n}\rho_{t(s)}^{n}G_{t(s)}^{n}}{N(s)}$$

**7. TD-Learning as SGD**

**Loss:**
$$l(\theta) = \frac{1}{2}\left( V^{\pi}(s;\theta) - [r + \gamma V^{\pi}(s^{\prime};\theta^{*})] \right)^{2}$$

**Gradient:**
$$\frac{\partial l}{\partial \theta} = \delta(s,a,s^{\prime}) \cdot \frac{\partial V^{\pi}(s;\theta)}{\partial \theta} \approx \delta(s,a,s^{\prime})$$
**Update:**
$$
\begin{aligned}
\theta &\leftarrow \theta - \alpha \delta \\
V(s) &\leftarrow V(s) + \alpha (r + \gamma V(s^{\prime}) - V(s))
\end{aligned}
$$

**Q-learning with linear function approximation**

Minimize the Mean Squared Error (MSE) between the current estimate and the TD target:
$$L(\theta) = \frac{1}{2} \left( y - Q(s,a;\theta) \right)^2$$

 **Target ($y$):** $r + \gamma \max_{a'} Q(s',a';\theta)$ (treated as a constant).

Calculate the gradient with respect to weights $\theta$:
$$\nabla_{\theta} L(\theta) = - \left( y - Q(s,a;\theta) \right) \nabla_{\theta} Q(s,a;\theta)$$

For **linear approximation** $Q(s,a;\theta) = \theta^{\top}\phi(s,a)$， the gradient is the feature vector:
$$\nabla_{\theta} Q(s,a;\theta) = \phi(s,a)$$

Apply SGD gradient descent $\theta \leftarrow \theta - \alpha \nabla_{\theta} L(\theta)$:

$$\theta \leftarrow \theta + \alpha \left( r + \gamma \max_{a'} Q(s',a';\theta) - \theta^{\top}\phi(s,a) \right) \phi(s,a)$$

